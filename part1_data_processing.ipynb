{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Cleaning and tokenizing the data. df_big refers to the whole 995K FakeNewsCorpus dataset and df refers to the sample. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from functools import reduce\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import FeatureHasher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('news_sample.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning data using the cleantext library\n",
    "def cleantext_clean(data):\n",
    "    #As clean did not have arguments for date we wrote our own using regular expressions \n",
    "    date_formats = [\n",
    "        r'\\d{2,4}/\\d{1,2}/\\d{2,4}(.*)',   # e.g 12/31/2021 or 12/31/21\n",
    "        r'\\d{2,4}-\\d{1,2}-\\d{2,4}(.*)',   # e.g 12-31-2021 or 12-31-21\n",
    "        ]\n",
    "\n",
    "    for date_format in date_formats:\n",
    "        data = re.sub(date_format, '<DATE>', str(data))\n",
    "\n",
    "    data = clean(data, lower=True, normalize_whitespace=True, no_urls=True, no_emails = True, no_numbers= True,\n",
    "    replace_with_url=\"<URL>\",\n",
    "    replace_with_email=\"<EMAIL>\",\n",
    "    replace_with_number=\"<NUM>\",)\n",
    "    return data\n",
    "    \n",
    "for column in df.columns:\n",
    "    df[column] = df[column].apply(cleantext_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the size of the vocabulary before processing\n",
    "def word_list(data): \n",
    "    word_list = []\n",
    "    for column in data.columns: \n",
    "        words = data[column].tolist()\n",
    "        words = [word for segments in words for word in str(segments).split()]\n",
    "        word_list = word_list + words\n",
    "    return word_list\n",
    "vc_size_before_processing = len(set(word_list(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords and stemming\n",
    "def remove_stopwords(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = str(data).split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def stem_input(data):\n",
    "    ps = PorterStemmer()\n",
    "    words = str(data).split()\n",
    "    return reduce(lambda x, y: x + ' ' + ps.stem(y), words, '')\n",
    "\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the size of the vocabulary after removing stopwords and stemming\n",
    "vc_size_no_stopwords = len(set(word_list(df)))\n",
    "print(vc_size_before_processing)\n",
    "print(vc_size_no_stopwords)\n",
    "\n",
    "reduction_rate_stopwords = ((vc_size_before_processing-vc_size_no_stopwords)/ vc_size_before_processing) * 100\n",
    "print(reduction_rate_stopwords, '%')\n",
    "\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].apply(stem_input)\n",
    "\n",
    "vc_size_stemming = len(set(word_list(df)))\n",
    "reduction_rate_stemming = ((vc_size_before_processing-vc_size_stemming) / vc_size_before_processing) * 100\n",
    "print(reduction_rate_stemming, '%')\n",
    "print(vc_size_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2, 3. df_big_cleaned refers to the cleaned big dataset, which we did locally and saved to a csv-file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big= pd.read_csv('995,000_rows.csv', index_col=0)\n",
    "df_big_cleaned = pd.read_csv('cleaned_dataset.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting how many NaN-values are in the author column\n",
    "nan_count = df_big['authors'].isna().sum()\n",
    "\n",
    "(nan_count/ len(df_big['authors'])) * 100\n",
    "\n",
    "df1 = df_big.copy()\n",
    "df1['type'] = df1['type'].replace(['bias','conspiracy','fake','hate','junksci','satire'],'fake')\n",
    "df1['type'] = df1['type'].replace(['political','reliable','clickbait'],'reliable')\n",
    "\n",
    "df_fake = df1[df1['type'] == 'fake']\n",
    "df_reliable = df1[df1['type'] == 'reliable']\n",
    "\n",
    "print (\"fake author nan %: \", ((df_fake['authors'].isna().sum()/ len(df_fake['authors']) * 100)))\n",
    "print (\"reliable author nan %: \", ((df_reliable['authors'].isna().sum()/ len(df_reliable['authors']) * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a pandas Series of the frequency of every word in the data to calculate how often each word appears\n",
    "def word_frequency(data_frame):\n",
    "    word_freq = {}\n",
    "    for column in data_frame.columns:\n",
    "        text = data_frame[column]\n",
    "        all_text = ' '.join(str(item) for item in text)\n",
    "        words = all_text.split()\n",
    "        for word in words:\n",
    "            if (word != \"<num>\") and (word != \"<num>,\") and (word != \"<num>.\") and (word != \"<num>:\") and (word != \"<url>\") and (word != \"<email>\") and (word != \"nan\") and (word != \"<date>\") and (word != \",\")and (word != \".\") and (word != \"-\") and (word != \"/\") and (word != \"['']\"):\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_word_freq\n",
    "\n",
    "frequencies_before = word_frequency(df_big)\n",
    "len(frequencies_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100 most frequent words before cleaning\n",
    "hundred_most_frequent_before = frequencies_before[:100]\n",
    "\n",
    "words, freqs = zip(*hundred_most_frequent_before)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(words, freqs)\n",
    "plt.xlabel('word')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Frequency of the 100 most used words in the dataset before cleaning')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100 most frequent words after cleaning \n",
    "frequencies_after = word_frequency(df_big_cleaned)\n",
    "hundred_most_frequent_after = frequencies_after[:100]\n",
    "\n",
    "words, freqs = zip(*hundred_most_frequent_after)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(words, freqs)\n",
    "plt.xlabel('word')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Frequency of the 100 most used words in the dataset after cleaning')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping articles by domain and counting the number of articles for each domain\n",
    "domain_counts = df_big['domain'].value_counts()\n",
    "domain_counts_df = pd.DataFrame(domain_counts.reset_index())\n",
    "domain_counts_df.columns = ['Domain', 'Count']\n",
    "\n",
    "#Creating a boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "domain_counts_df.boxplot(column='Count', vert=False)\n",
    "plt.title('Domain Distribution Boxplot')\n",
    "plt.xlabel('Number of Articles')\n",
    "plt.yticks([])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "domain_counts_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the distribution of domains \n",
    "domain_distribution = df_big_cleaned['domain'].value_counts()\n",
    "percentage_distribution = domain_distribution / domain_distribution.sum() * 100\n",
    "print(percentage_distribution)\n",
    "\n",
    "type_distribution = df_big_cleaned['type'].value_counts()\n",
    "percentage_distribution = type_distribution / type_distribution.sum() * 100\n",
    "print(percentage_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into training, validation and test data\n",
    "x=df_big_cleaned.drop(columns=['type'])\n",
    "y=df_big_cleaned['type']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_test, y_test, test_size=0.5,random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
