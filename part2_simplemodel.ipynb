{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliagrundemar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from functools import reduce\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big_cleaned = pd.read_csv('cleaned_dataset.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "reliable    52.238806\n",
      "fake        47.761194\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Omitting 'unknown', 'unreliable' and 'rumor' types and dropping nan values \n",
    "df_big_cleaned = df_big_cleaned.dropna(subset=['type'])\n",
    "df_big_cleaned = df_big_cleaned[df_big_cleaned['type'] != 'unknown']\n",
    "df_big_cleaned = df_big_cleaned[df_big_cleaned['type'] != 'unreliable']\n",
    "df_big_cleaned = df_big_cleaned[df_big_cleaned['type'] != 'rumor']\n",
    "\n",
    "#Grouping the types 'bias','clickbait','conspiracy','fake','hate','junksci','unreliable' into 'fake'\n",
    "df_big_cleaned['type'] = df_big_cleaned['type'].replace(['bias','conspiracy','fake','hate','junksci','satire'],'fake')\n",
    "\n",
    "#Grouping the types 'political','reliable','clickbait' into 'reliable'\n",
    "df_big_cleaned['type'] = df_big_cleaned['type'].replace(['political','reliable','clickbait'],'reliable')\n",
    "\n",
    "type_distribution = df_big_cleaned['type'].value_counts()\n",
    "percentage_distribution = type_distribution / type_distribution.sum() * 100\n",
    "print(percentage_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training, validation and test sets\n",
    "x=df_big_cleaned.drop(columns=['type'])\n",
    "y=df_big_cleaned['type']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_test, y_test, test_size=0.5,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Training the model on the content of the articles\n",
    "x_train_content = x_train['content']\n",
    "x_train_content = x_train_content.fillna(\"nan\")\n",
    "x_validation_content = x_validation['content']\n",
    "x_validation_content = x_validation_content.fillna(\"nan\")\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "x_train_content = vectorizer.fit_transform(x_train_content)\n",
    "x_validation_content = vectorizer.transform(x_validation_content)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "model = LogisticRegression(solver= 'sag',max_iter=10000)\n",
    "model.fit(x_train_content, y_train)\n",
    "\n",
    "y_pred = model.predict(x_validation_content)\n",
    "\n",
    "acc = accuracy_score(y_validation, y_pred)\n",
    "\n",
    "print(acc)\n",
    "\n",
    "with open('trained_model_content.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.775\n"
     ]
    }
   ],
   "source": [
    "#Training the model on the authors and content of the articles\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "x_train_authors = x_train['authors']\n",
    "x_val_authors = x_validation['authors']\n",
    "\n",
    "x_train_authors = x_train_authors.fillna(\"nan\")\n",
    "x_val_authors = x_val_authors.fillna(\"nan\")\n",
    "\n",
    "#Checking that each entry in the 'authors' column is a string\n",
    "x_train_authors = x_train_authors.apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "x_val_authors = x_val_authors.apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "#Converting to DataFrame\n",
    "df_authors = pd.DataFrame({'authors': x_train_authors})\n",
    "df_val_authors = pd.DataFrame({'authors': x_val_authors})\n",
    "\n",
    "#Initializing FeatureHasher\n",
    "hasher = FeatureHasher(n_features=7500, input_type='string')\n",
    "\n",
    "#Hash encode 'authors' column\n",
    "hashed_features_train_author = hasher.fit_transform(df_authors['authors'])\n",
    "hashed_features_val_author = hasher.fit_transform(df_val_authors['authors'])\n",
    "\n",
    "#Converting hashed features to dataframe\n",
    "hashed_df = pd.DataFrame(hashed_features_train_author.toarray(), columns=[f'author_hash_{i}' for i in range(7500)])\n",
    "hashed_df_val = pd.DataFrame(hashed_features_val_author.toarray(), columns=[f'author_hash_{i}' for i in range(7500)])\n",
    "\n",
    "combined_train_features = hstack([x_train_content, hashed_features_train_author])\n",
    "combined_val_features = hstack([x_validation_content, hashed_features_val_author])\n",
    "\n",
    "#Initializing logistic regression model\n",
    "model2 = LogisticRegression(max_iter=2000)\n",
    "\n",
    "model2.fit(combined_train_features, y_train)\n",
    "\n",
    "#Predicting on the test set\n",
    "y_pred = model2.predict(combined_val_features)\n",
    "\n",
    "#Evaluating performance \n",
    "accuracy = accuracy_score(y_validation, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "with open('trained_model2.pkl', 'wb') as f:\n",
    "    pickle.dump(model2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7875\n"
     ]
    }
   ],
   "source": [
    "#Training the model on the domains and contents of the articles \n",
    "\n",
    "#Hasing over domains\n",
    "x_train_domain = x_train['domain']\n",
    "x_val_domain = x_validation['domain']\n",
    "x_train_domain = x_train_domain.fillna(\"nan\")\n",
    "x_val_domain = x_val_domain.fillna(\"nan\")\n",
    "\n",
    "#Concatenating domain data from both training and validation datasets\n",
    "combined_domains = pd.concat([x_train_domain, x_val_domain], ignore_index=True)\n",
    "\n",
    "#Converting domain data to list of lists\n",
    "combined_domains = combined_domains.apply(lambda x: [x] if isinstance(x, str) else x).tolist()\n",
    "\n",
    "#Initializing FeatureHasher\n",
    "hasher = FeatureHasher(n_features=500, input_type='string')\n",
    "\n",
    "#Transforming combined domain data\n",
    "hashed_features = hasher.fit_transform(combined_domains)\n",
    "\n",
    "#Splitting hashed features back into training and validation parts\n",
    "hashed_features_train = hashed_features[:len(x_train_domain)]\n",
    "hashed_features_val = hashed_features[len(x_train_domain):]\n",
    "\n",
    "#Combining hashed features with content features\n",
    "combined_train_features = hstack([x_train_content, hashed_features_train])\n",
    "combined_val_features = hstack([x_validation_content, hashed_features_val])\n",
    "\n",
    "#Initializing logistic regression model\n",
    "model3 = LogisticRegression(max_iter=2000)\n",
    "\n",
    "model3.fit(combined_train_features, y_train)\n",
    "\n",
    "#Predicting on the validation set\n",
    "y_pred = model3.predict(combined_val_features)\n",
    "\n",
    "#Evaluating performance\n",
    "accuracy = accuracy_score(y_validation, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "with open('trained_model3.pkl', 'wb') as f:\n",
    "    pickle.dump(model3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7625\n"
     ]
    }
   ],
   "source": [
    "#Training the model on contents, authors and domains of the articles\n",
    "\n",
    "#Combining hashed domain and author features with content features\n",
    "combined_train_features = hstack([x_train_content, hashed_features_train, hashed_features_train_author])\n",
    "combined_val_features = hstack([x_validation_content, hashed_features_val, hashed_features_val_author])\n",
    "\n",
    "#Initializing logistic regression model\n",
    "model4 = LogisticRegression(solver= 'sag', max_iter=10000)\n",
    "\n",
    "model4.fit(combined_train_features, y_train)\n",
    "\n",
    "#Predicting on the validation set\n",
    "y_pred = model4.predict(combined_val_features)\n",
    "\n",
    "#Evaluating performance\n",
    "accuracy = accuracy_score(y_validation, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "with open('trained_model4.pkl', 'wb') as f:\n",
    "    pickle.dump(model4, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4782, 5)\n"
     ]
    }
   ],
   "source": [
    "#Adding the extra reliable data to the dataset\n",
    "reliable = pd.read_csv('reliable_scraped_data.csv')\n",
    "reliable['type'] = 'reliable'\n",
    "\n",
    "print(reliable.shape)\n",
    "concatenated_data = pd.concat([df_big_cleaned,reliable],axis=0)\n",
    "\n",
    "x=concatenated_data.drop(columns=['type'])\n",
    "y=concatenated_data['type']\n",
    "x_train_concat, x_test_concat, y_train_concat, y_test_concat = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "x_validation_concat, x_test_concat, y_validation_concat, y_test_concat = train_test_split(x_test_concat, y_test_concat, test_size=0.5,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9445438282647585\n"
     ]
    }
   ],
   "source": [
    "#Training the model with the extra reliable data on the content of the articles\n",
    "x_train_concat_content = x_train_concat['content']\n",
    "x_train_concat_content = x_train_concat_content.fillna(\"nan\")\n",
    "x_validation_concat_content = x_validation_concat['content']\n",
    "x_validation_concat_content = x_validation_concat_content.fillna(\"nan\")\n",
    "\n",
    "vectorizer_concat = CountVectorizer()\n",
    "\n",
    "x_train_concat_content = vectorizer_concat.fit_transform(x_train_concat_content)\n",
    "x_validation_concat_content = vectorizer_concat.transform(x_validation_concat_content)\n",
    "\n",
    "with open('vectorizer_concat.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "#Initializing logistic regression model\n",
    "model5 = LogisticRegression(solver= 'sag',max_iter=10000)\n",
    "model5.fit(x_train_concat_content, y_train_concat)\n",
    "\n",
    "#Predicting on the validation set\n",
    "y_pred_concat = model5.predict(x_validation_concat_content)\n",
    "\n",
    "#Evaluating performance\n",
    "acc = accuracy_score(y_validation_concat, y_pred_concat)\n",
    "\n",
    "print(acc)\n",
    "\n",
    "with open('trained_model_content_concat.pkl', 'wb') as f:\n",
    "    pickle.dump(model5, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
