{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from functools import reduce\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big_cleaned = pd.read_csv('cleaned_dataset.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Omitting 'unknown', 'unreliable' and 'rumor' types and dropping nan values \n",
    "df_big_cleaned = df_big_cleaned.dropna(subset=['type'])\n",
    "df_big_cleaned = df_big_cleaned[df_big_cleaned['type'] != 'unknown']\n",
    "df_big_cleaned = df_big_cleaned[df_big_cleaned['type'] != 'unreliable']\n",
    "df_big_cleaned = df_big_cleaned[df_big_cleaned['type'] != 'rumor']\n",
    "\n",
    "#Grouping the types 'bias','clickbait','conspiracy','fake','hate','junksci','unreliable' into 'fake'\n",
    "df_big_cleaned['type'] = df_big_cleaned['type'].replace(['bias','conspiracy','fake','hate','junksci','satire'],'fake')\n",
    "\n",
    "#Grouping the types 'political','reliable','clickbait' into 'reliable'\n",
    "df_big_cleaned['type'] = df_big_cleaned['type'].replace(['political','reliable','clickbait'],'reliable')\n",
    "\n",
    "type_distribution = df_big_cleaned['type'].value_counts()\n",
    "percentage_distribution = type_distribution / type_distribution.sum() * 100\n",
    "print(percentage_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training, validation and test sets\n",
    "x=df_big_cleaned.drop(columns=['type'])\n",
    "y=df_big_cleaned['type']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_test, y_test, test_size=0.5,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Training the model on the content of the articles\n",
    "x_train_content = x_train['content']\n",
    "x_train_content = x_train_content.fillna(\"nan\")\n",
    "x_validation_content = x_validation['content']\n",
    "x_validation_content = x_validation_content.fillna(\"nan\")\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "x_train_content = vectorizer.fit_transform(x_train_content)\n",
    "x_validation_content = vectorizer.transform(x_validation_content)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "model = LogisticRegression(solver= 'sag',max_iter=10000)\n",
    "model.fit(x_train_content, y_train)\n",
    "\n",
    "y_pred = model.predict(x_validation_content)\n",
    "\n",
    "acc = accuracy_score(y_validation, y_pred)\n",
    "\n",
    "print(acc)\n",
    "\n",
    "with open('trained_model_content.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model on the authors and content of the articles\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "x_train_authors = x_train['authors']\n",
    "x_val_authors = x_validation['authors']\n",
    "\n",
    "x_train_authors = x_train_authors.fillna(\"nan\")\n",
    "x_val_authors = x_val_authors.fillna(\"nan\")\n",
    "\n",
    "#Checking that each entry in the 'authors' column is a string\n",
    "x_train_authors = x_train_authors.apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "x_val_authors = x_val_authors.apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "#Converting to DataFrame\n",
    "df_authors = pd.DataFrame({'authors': x_train_authors})\n",
    "df_val_authors = pd.DataFrame({'authors': x_val_authors})\n",
    "\n",
    "#Initializing FeatureHasher\n",
    "hasher = FeatureHasher(n_features=7500, input_type='string')\n",
    "\n",
    "#Hash encode 'authors' column\n",
    "hashed_features_train_author = hasher.fit_transform(df_authors['authors'])\n",
    "hashed_features_val_author = hasher.fit_transform(df_val_authors['authors'])\n",
    "\n",
    "#Converting hashed features to dataframe\n",
    "hashed_df = pd.DataFrame(hashed_features_train_author.toarray(), columns=[f'author_hash_{i}' for i in range(7500)])\n",
    "hashed_df_val = pd.DataFrame(hashed_features_val_author.toarray(), columns=[f'author_hash_{i}' for i in range(7500)])\n",
    "\n",
    "combined_train_features = hstack([x_train_content, hashed_features_train_author])\n",
    "combined_val_features = hstack([x_validation_content, hashed_features_val_author])\n",
    "\n",
    "#Initializing logistic regression model\n",
    "model2 = LogisticRegression(max_iter=2000)\n",
    "\n",
    "model2.fit(combined_train_features, y_train)\n",
    "\n",
    "#Predicting on the test set\n",
    "y_pred = model2.predict(combined_val_features)\n",
    "\n",
    "#Evaluating performance \n",
    "accuracy = accuracy_score(y_validation, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "with open('trained_model2.pkl', 'wb') as f:\n",
    "    pickle.dump(model2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the extra reliable data to the dataset\n",
    "reliable = pd.read_csv('reliable_scraped_data.csv')\n",
    "reliable['type'] = 'reliable'\n",
    "\n",
    "print(reliable.shape)\n",
    "concatenated_data = pd.concat([df_big_cleaned,reliable],axis=0)\n",
    "\n",
    "x=concatenated_data.drop(columns=['type'])\n",
    "y=concatenated_data['type']\n",
    "x_train_concat, x_test_concat, y_train_concat, y_test_concat = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "x_validation_concat, x_test_concat, y_validation_concat, y_test_concat = train_test_split(x_test_concat, y_test_concat, test_size=0.5,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model with the extra reliable data on the content of the articles\n",
    "x_train_concat_content = x_train_concat['content']\n",
    "x_train_concat_content = x_train_concat_content.fillna(\"nan\")\n",
    "x_validation_concat_content = x_validation_concat['content']\n",
    "x_validation_concat_content = x_validation_concat_content.fillna(\"nan\")\n",
    "x_test_concat_content = x_test_concat['content']\n",
    "x_test_concat_content = x_test_concat_content.fillna(\"nan\")\n",
    "\n",
    "vectorizer_concat = CountVectorizer()\n",
    "\n",
    "x_train_concat_content = vectorizer_concat.fit_transform(x_train_concat_content)\n",
    "x_validation_concat_content = vectorizer_concat.transform(x_validation_concat_content)\n",
    "x_test_concat_content = vectorizer_concat.transform(x_test_concat_content)\n",
    "\n",
    "with open('vectorizer_concat.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "#Initializing logistic regression model\n",
    "model5 = LogisticRegression(solver= 'sag',max_iter=10000)\n",
    "model5.fit(x_train_concat_content, y_train_concat)\n",
    "\n",
    "#Predicting on the validation set\n",
    "y_pred_concat = model5.predict(x_validation_concat_content)\n",
    "\n",
    "#Evaluating performance\n",
    "acc = accuracy_score(y_validation_concat, y_pred_concat)\n",
    "print(acc)\n",
    "\n",
    "with open('trained_model_content_concat.pkl', 'wb') as f:\n",
    "    pickle.dump(model5, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Evaluation of the simple model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to make confusion matrix\n",
    "import seaborn as sns\n",
    "def make_confusion_matrix(cf, group_names=None, categories='auto', count=True, percent=True, cbar=True, xyticks=True,\n",
    "                          xyplotlabels=True, sum_stats=True, figsize=None, cmap='Blues', title=None):\n",
    "\n",
    "    # Function to generate text inside each square\n",
    "    def generate_labels(cf, count, percent):\n",
    "        blanks = ['' for _ in range(cf.size)]\n",
    "        if group_names and len(group_names) == cf.size:\n",
    "            group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "        else:\n",
    "            group_labels = blanks\n",
    "\n",
    "        if count:\n",
    "            group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "        else:\n",
    "            group_counts = blanks\n",
    "\n",
    "        if percent:\n",
    "            group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten() / np.sum(cf)]\n",
    "        else:\n",
    "            group_percentages = blanks\n",
    "\n",
    "        box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in\n",
    "                      zip(group_labels, group_counts, group_percentages)]\n",
    "        return np.asarray(box_labels).reshape(cf.shape[0], cf.shape[1])\n",
    "\n",
    "    #Generating labels\n",
    "    box_labels = generate_labels(cf, count, percent)\n",
    "\n",
    "    #Setting figure parameters according to other arguments\n",
    "    if figsize is None:\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if not xyticks:\n",
    "        categories = False\n",
    "\n",
    "    colors = ['Reds', 'Greens']\n",
    "\n",
    "    #Make the heatmap visualization\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf, annot=box_labels, fmt=\"\", cmap=cmap, cbar=cbar, xticklabels=categories, yticklabels=categories, \n",
    "                mask=cf == 0)  # Mask zeros to avoid displaying empty cells\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating accuracy on the test set and generating an f1-score and confusion matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "#Evaluating on the test set \n",
    "y_pred_test = model5.predict(x_test_concat_content)\n",
    "acc_test = accuracy_score(y_test_concat, y_pred_test)\n",
    "print('accuracy:', acc_test)\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test_concat, y_pred_concat, pos_label='reliable')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_concat, y_pred_concat)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix) \n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['Fake', 'Real']\n",
    "\n",
    "make_confusion_matrix(conf_matrix, group_names=labels, categories=categories, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
